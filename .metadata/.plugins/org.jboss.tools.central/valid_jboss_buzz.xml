<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Integrate your Quarkus application with GPT4All</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/25/integrate-your-quarkus-application-gpt4all" /><author><name>Alex Soto Bueno</name></author><id>4930e5a1-8b40-4357-88db-302eae57eb71</id><updated>2023-09-25T07:00:00Z</updated><published>2023-09-25T07:00:00Z</published><summary type="html">&lt;p&gt;ChatGPT is a language model developed by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt;, part of the GPT (Generative Pre-trained Transformer) series of models. It is designed to generate human-like text based on the input it receives. However, there are alternatives. Some open source options are meant to run locally/on-premises instead of in a hosted solution, making them perfect for controlling your data so that it doesn't go outside your boundaries or out of your control.&lt;/p&gt; &lt;p&gt;One of these projects is &lt;a href="https://gpt4all.io/"&gt;GPT4All&lt;/a&gt;. GPT4All is an ecosystem to train and deploy powerful and customized large language models (LLM) that run locally on a standard machine with no special features, such as a GPU.&lt;/p&gt; &lt;p&gt;This article will demonstrate how to integrate GPT4All into a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; application so that you can query this service and return a response without any external resources.&lt;/p&gt; &lt;h2&gt;Install GPT4All&lt;/h2&gt; &lt;p&gt;The first thing you need to do is install GPT4All on your computer. This step is essential because it will download the trained model for our application. A GPT4All model is a 3GB - 8GB size file that is integrated directly into the software you are developing.&lt;/p&gt; &lt;p&gt;GPT4All provides specific bindings for different languages on top of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C/C++&lt;/a&gt; model backends. Currently, the following language bindings are provided:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;C#&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;TypeScript&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To install GPT4All, navigate to &lt;a href="https://gpt4all.io/index.html"&gt;https://gpt4all.io/index.html&lt;/a&gt;, select your operating system, and download the executable file.&lt;/p&gt; &lt;p&gt;Figure 1 shows the GPT4All homepage with all of the available installation options.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_21.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_21.png?itok=V2btjPV_" width="600" height="324" alt="The GPT4All homepage." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The GPT4All home page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once you have downloaded the application, install and open the program. This application, in addition to being used to use the models without programming skills (it's like the OpenAI Chat GPT chat interface), is used to download and manage the different available models.&lt;/p&gt; &lt;h2&gt;Download the data model&lt;/h2&gt; &lt;p&gt;The first thing you'll see when starting the application is a window that lets you decide on the data model to download. In this case, choose &lt;strong&gt;GPT4All Falcon&lt;/strong&gt; and click the &lt;strong&gt;Download&lt;/strong&gt; button. This process might take some time, but in the end, you'll end up with the model downloaded. This model is an Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions, including word problems, multi-turn dialogue, code, poems, songs, and stories.&lt;/p&gt; &lt;p&gt;Figure 2 depicts the pop-up window shown the first time you start the GPT4All application.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2_10.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image2_10.png?itok=hITeF1Dq" width="600" height="344" alt="GPT4all popup window displays available models to choose from." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Choosing the GPT4All Falcon data model to download.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once the download process is complete, the model will be presented on the local disk. The location is displayed next to the &lt;strong&gt;Download Path&lt;/strong&gt; field, as shown in Figure 3—we'll need this later in the tutorial.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_12.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image3_12.png?itok=nqmCdqPB" width="600" height="344" alt="The download path field marked in the lower-left corner of the GPT4All interface." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The download path location in GPT4All.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;With the model in place, let's scaffold the Quarkus application.&lt;/p&gt; &lt;h2&gt;Build the Quarkus application&lt;/h2&gt; &lt;p&gt;First, go to &lt;a href="https://code.quarkus.io/"&gt;https://code.quarkus.io/&lt;/a&gt; and generate a new Quarkus application with RESTEasy and the Reactive Jackson extension. You can pre-populate these options with all necessary dependencies by clicking &lt;a href="https://code.quarkus.io/?a=quarkus-gpt&amp;e=resteasy-reactive-jackson"&gt;this link&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 4 shows the Quarkus generator page.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image4_9.png?itok=pyGIVP0P" width="600" height="274" alt="The RESTEasy Reactive Jackson extension is marked on the Quarkus generator page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Scaffolding a new Quarkus application with RESTEasy and the Reactive Jackson extension.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Click the &lt;strong&gt;Generate the application&lt;/strong&gt; button, download the zip file, and unzip it.&lt;/p&gt; &lt;p&gt;Open the unzipped project in your desired integrated development environment (IDE). In the following section, you'll add the &lt;a href="https://github.com/nomic-ai/gpt4all/tree/main/gpt4all-bindings/java"&gt;GPT4All Java&lt;/a&gt; binding dependency and modify the REST endpoint, adding the integration code between Quarkus and GPT4All.&lt;/p&gt; &lt;h2&gt;Implement the REST API&lt;/h2&gt; &lt;p&gt;We want to implement a REST service that receives a question as body content, uses the GPT4All local model to find the answer, and then responds with a message back to the caller.&lt;/p&gt; &lt;p&gt;Before adding any new code, rename the default path definition from &lt;code&gt;@Path("/hello")&lt;/code&gt; to &lt;code&gt;@Path("/interact")&lt;/code&gt; to show precisely the purpose of the endpoint.&lt;/p&gt; &lt;p&gt;Next, start by defining the REST method signature by opening the &lt;code&gt;GreetingResource.java&lt;/code&gt; file and creating the following method:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;import jakarta.ws.rs.POST; @POST @Consumes(MediaType.TEXT_PLAIN) @Produces(MediaType.TEXT_PLAIN) public String hello(String content) { }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This method will contain all the logic necessary for interacting with GPT4All and add the GPT4All Java bindings in the classpath.&lt;/p&gt; &lt;h2&gt;Add GPT4All Java bindings&lt;/h2&gt; &lt;p&gt;Open the &lt;code&gt;pom.xml&lt;/code&gt; file and add the following dependencies:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;dependency&gt;    &lt;groupId&gt;com.hexadevlabs&lt;/groupId&gt;    &lt;artifactId&gt;gpt4all-java-binding&lt;/artifactId&gt;    &lt;version&gt;1.1.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt;   &lt;groupId&gt;org.slf4j&lt;/groupId&gt;   &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;   &lt;version&gt;1.7.36&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first dependency is the Java binding library to interact with the model from Java code; the second is not mandatory but is used as a logging framework by GPT4All.&lt;/p&gt; &lt;p&gt;Next, before you can query the model, you need to configure and initialize the Java binding classes. Open the &lt;code&gt;GreetingResource.java&lt;/code&gt; class again, copy the following code, which will configure the location and file containing the data model (recall that in the previous section, we mentioned that this directory was important), and initialize everything.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;// Import required  import com.hexadevlabs.gpt4all.LLModel; // Quarkus configuration properties specifying the location of the data model @ConfigProperty(name = "model.path") String baseModelPath; @ConfigProperty(name = "model.file") String modelFilePath; // GPT4All facade classes LLModel model; LLModel.GenerationConfig config; // Configures and Initialize the model @PostConstruct public void initModel() {     java.nio.file.Path modelPath = java.nio.file.Path.of(baseModelPath, modelFilePath);     model = new LLModel(modelPath);     config = LLModel.config()             .withNPredict(4096).build(); } // When the application is shutting down, the model is closed @PreDestroy public void cleanModel() throws Exception {     model.close(); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step is to fill in the hello method with the integration code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;// Creates the object required by GPT4All to send the question final List&lt;Map&lt;String, String&gt;&gt; message = List.of(createMessage(content)); // Sends the question to the model final LLModel.ChatCompletionResponse chatCompletionResponse =        model.chatCompletion(message, config); // Returns the response return chatCompletionResponse.choices.toString(); The full version of the GreetingResource.java class is shown in the following snippet: package org.acme; import com.hexadevlabs.gpt4all.LLModel; import jakarta.annotation.PostConstruct; import jakarta.annotation.PreDestroy; import jakarta.ws.rs.Consumes; import jakarta.ws.rs.POST; import jakarta.ws.rs.Path; import jakarta.ws.rs.Produces; import jakarta.ws.rs.core.MediaType; import java.util.List; import java.util.Map; import org.eclipse.microprofile.config.inject.ConfigProperty; @Path("/interact") public class GreetingResource {     @ConfigProperty(name = "model.path")     String baseModelPath;     @ConfigProperty(name = "model.file")     String modelFilePath;     LLModel model;     LLModel.GenerationConfig config;     @PostConstruct     public void initModel() {         java.nio.file.Path modelPath = java.nio.file.Path.of(baseModelPath, modelFilePath);         model = new LLModel(modelPath);         config = LLModel.config()             .withNPredict(4096).build();     }     @PreDestroy     public void cleanModel() throws Exception {         model.close();     }     @POST     @Consumes(MediaType.TEXT_PLAIN)     @Produces(MediaType.TEXT_PLAIN)     public String hello(String content) {         // Creates the object required by GPT4All to send the question         final List&lt;Map&lt;String, String&gt;&gt; message = List.of(createMessage(content));         // Sends the question to the model         final LLModel.ChatCompletionResponse chatCompletionResponse =             model.chatCompletion(message, config);         // Returns the response         return chatCompletionResponse.choices.toString();     }     private Map&lt;String, String&gt; createMessage(String content) {         return Map.of("role", "user", "content", content);     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before deploying the solution, the last thing to do is to configure the location parameters in the &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Model file, if you download another model, then the file is a different one model.file=ggml-model-gpt4all-falcon-q4_0.bin # Directory where model is stored model.path=/Users/asotobu/Library/Application Support/nomic.ai/GPT4All&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Run the application&lt;/h2&gt; &lt;p&gt;We will package and start the application like any other Java application. Open a terminal window and run the following command from the root directory of the project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./mvnw clean package -DskipTests&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the process finishes, start the application by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;java -Xms8G -Xmx8G -jar target/quarkus-app/quarkus-run.jar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In another terminal window, let's send a request to our service asking to code a Java class that encodes a string into Base64:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -X 'POST' \   'http://localhost:8080/hello' \   -H 'accept: text/plain' \   -H 'Content-Type: text/plain' \   -d 'write a Java program that encodes to base64' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This operation can take seconds or minutes, depending on the amount of memory, CPUs, and JVM heap defined when starting the application, etc.. Some patience might be necessary.&lt;/p&gt; &lt;p&gt;The output should be similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[{role=assistant, content=&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's a sample Java program that encodes a string to Base64:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;java import java.util.Base64; public class Base64Encoder {     public static void main(String[] args) throws Exception {         String str = "Hello World!";         String base64Encoded = Base64.encodeToString(str.getBytes(), Base64.DEFAULT);         System.out.println(base64Encoded);     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This program uses the &lt;code&gt;Base64&lt;/code&gt; class from the &lt;code&gt;java.util&lt;/code&gt; package to encode the string &lt;code&gt;"Hello World!"&lt;/code&gt; to Base64. The &lt;code&gt;Base64.encodeToString&lt;/code&gt; method takes the string to be encoded and the encoding algorithm (in this case, &lt;code&gt;Base64.DEFAULT&lt;/code&gt;) as arguments and returns the base64-encoded string.&lt;/p&gt; &lt;p&gt;Note that the &lt;code&gt;Base64&lt;/code&gt; class can be used to decode Base64-encoded strings as well.&lt;/p&gt; &lt;p&gt;Pretty impressive so far—the application is running on your local machine with no external party involved.&lt;/p&gt; &lt;h2&gt;Parse the output&lt;/h2&gt; &lt;p&gt;The output is in Markdown format, with the code provided within a code block, explaining the function of the code and some hints as text. To generate source code files automatically, you must parse the content and extract just the Java code block.&lt;/p&gt; &lt;p&gt;One of the Java libraries you can use to parse and extract data from Markdown documents is &lt;a href="https://github.com/commonmark/commonmark-java"&gt;commonmark&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Open the &lt;code&gt;pom.xml&lt;/code&gt; file and add the following dependency:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;&lt;dependency&gt;   &lt;groupId&gt;org.commonmark&lt;/groupId&gt;   &lt;artifactId&gt;commonmark&lt;/artifactId&gt;   &lt;version&gt;0.20.0&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This library uses the &lt;a href="https://en.wikipedia.org/wiki/Visitor_pattern"&gt;visitor pattern&lt;/a&gt; to navigate through the document blocks and extract the required information; in this case, the Java block containing the encoding to Base64 logic. With the Java source code extracted, you could create a file, return the content to the caller, or parse the Java code to modify any part. &lt;/p&gt; &lt;p&gt;Let's modify the previous class to retrieve and parse the answer given by GPT4All and extract only the Java source code block instead of the complete response.&lt;/p&gt; &lt;p&gt;Start by creating a class implementing the Visitor pattern, visiting only source code blocks. When a code block is found, validate it's a Java block and store its content into a variable:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;import org.commonmark.node.AbstractVisitor; import org.commonmark.node.FencedCodeBlock; import java.util.Optional; static class JavaCodeBlockVisitor extends AbstractVisitor {     private String sourceCode;     public Optional&lt;String&gt; getGeneratedSourceCode() {         return Optional.ofNullable(sourceCode);     }     @Override     // Only visit code blocks     public void visit(FencedCodeBlock code) {         // If it's a Java block         if ("java".equals(code.getInfo())) {             // Get the content of the block             sourceCode = code.getLiteral();         }     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, modify the hello method to get the content from the GPT4All API instead of returning it directly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;import java.util.List; import java.util.Map; // Returns the response final List&lt;Map&lt;String, String&gt;&gt; choices = chatCompletionResponse.choices; // Check if there is a response if (choices.isEmpty()) {     throw new IllegalStateException("No Java code"); } // Gets the content in Markdown format String text = choices.get(0).get("content");&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need to use the Markdown parser to navigate through all the blocks using the previous visitor class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;import org.commonmark.node.Node; import org.commonmark.parser.Parser; final Parser parser = Parser.builder().build(); // Parse the content final Node node = parser.parse(text); // Navigate through the model, finding Java blocks JavaCodeBlockVisitor javaCodeBlockVisitor = new JavaCodeBlockVisitor(); node.accept(javaCodeBlockVisitor);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before rerunning the example, change the return content to the one extracted within the &lt;code&gt;JavaCodeBlockVisitor&lt;/code&gt; class:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;// Return the Java code return javaCodeBlockVisitor.getGeneratedSourceCode().orElseThrow(() -&gt; new IllegalArgumentException("No code found"));&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Build and package the application, repeat the same request as was executed previously, and notice that only the source code is returned, not the complete response.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article showed how you can use GPT models without depending on external services like OpenAI ChatGPT. If you have privacy concerns but still want to use AI to generate some Java code, then the GPT4All project is a good choice as it runs within a localized environment.&lt;/p&gt; &lt;p&gt;Moreover, GPT4All lets you train your model or improve an existing model with your data. Since you execute it locally, the data is kept from everyone. The interaction with Java is straightforward, with no complex steps; all you need to do is load the library and start interacting with the model. &lt;/p&gt; &lt;p&gt;Finally, you've seen a library to parse Markdown documents in Java, a handy toolkit for extracting information from Markdown content.&lt;/p&gt; &lt;p&gt;GPT4All opens a new door in the AI revolution, letting you use AI without depending on cloud instances or closed models. GPT4All lets you train, deploy, and use AI privately without depending on external service providers.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/25/integrate-your-quarkus-application-gpt4all" title="Integrate your Quarkus application with GPT4All"&gt;Integrate your Quarkus application with GPT4All&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alex Soto Bueno</dc:creator><dc:date>2023-09-25T07:00:00Z</dc:date></entry><entry><title>The Red Hat community commitment to open source</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/21/red-hat-community-commitment-open-source" /><author><name>Mithun T. Dhar</name></author><id>b46be16a-3b92-4988-b8e6-5a089c0369a5</id><updated>2023-09-21T13:00:00Z</updated><published>2023-09-21T13:00:00Z</published><summary type="html">&lt;p&gt;The increasing adoption of &lt;a href="https://developers.redhat.com/topics/open-source-communities"&gt;open source&lt;/a&gt; software underscores its tremendous value and impact. According to &lt;a href="https://blog.gitnux.com/open-source-software-statistics/#:~:text=78%25%20of%20businesses%20are%20using%20open%2Dsource%20software.&amp;text=This%20statistic%20is%20an%20important,software%20in%20the%20business%20world."&gt;Gitnux&lt;/a&gt;, 78% of businesses use open source technologies, and about 96% of current applications contain at least one open source component. Developers also recognize the value of open source software in enhancing their skill sets and enabling them to collaborate on projects that push technological boundaries.&lt;/p&gt; &lt;p&gt;Red Hat is the world’s largest open source company and a leading contributor to the Cloud Native Computing Foundation (&lt;a href="https://www.cncf.io/"&gt;CNCF&lt;/a&gt;). We strive to develop products and technologies that advance both the open source community and the digital landscape.&lt;/p&gt; &lt;p&gt;Our extensive range of open source solutions encompasses operating systems, middleware, storage, container services, and developer tools and products like &lt;a href="https://developers.redhat.com/products/developer-hub/overview"&gt;Red Hat Developer Hub&lt;/a&gt;, &lt;a href="https://developers.redhat.com/developer-sandbox/"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, etc. We also create and contribute to numerous communities and projects while upholding the core &lt;a href="https://www.redhat.com/en/topics/open-source/what-is-open-source#values-of-open-source"&gt;values&lt;/a&gt; that drive open source innovation. These communities foster an ecosystem where developers and nontechnical individuals can collaborate, provide and access mentorship, and create top-tier products using open source technology.&lt;/p&gt; &lt;p&gt;Read on to explore our approach to open source and learn about the projects and contributions advancing our technological landscape.&lt;/p&gt; &lt;h2&gt;Red Hat open source approach&lt;/h2&gt; &lt;p&gt;Red Hat’s decades-long commitment to open source software includes an open development model. This model connects engineers with broader communities, facilitating the creation of secure, reliable products that consistently challenge conventional boundaries.&lt;/p&gt; &lt;p&gt;We’ve also founded several upstream communities, providing support beyond code contribution, participation, and engagement. We integrate with other upstream projects to develop commercialized, industry-standard projects within a service-rich ecosystem. This open development approach lowers application development costs and enhances the security of our products.&lt;/p&gt; &lt;p&gt;Most recently, we introduced &lt;a href="https://developers.redhat.com/products/developer-hub/overview"&gt;Red Hat Developer Hub&lt;/a&gt;, an internal portal packed with tools that expedite development processes. Building on Backstage, a CNCF project, Developer Hub helps create and customize developer portals and offers numerous Backstage plug-ins. These plug-ins include Keycloak for authorization and authentication and Topology for visualizing real-time workload health in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Red Hat Developer Hub offers several advantages, including:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Improved work efficiency: &lt;/strong&gt;Developer Hub contains all the tools developers and teams require, from clusters and templates to integrated development environments (IDEs), GitHub repos, and documentation. This integrated environment saves time configuring and switching between tools and programs. Less distraction means greater freedom to innovate and shortened time to deployment.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Reduced knowledge barriers via self-service:&lt;/strong&gt; Developer Hub contains &lt;a href="https://cloud.redhat.com/blog/designing-golden-paths"&gt;Golden Path&lt;/a&gt; templates, a collection of pre-architected, well-defined steps for building software without in-depth knowledge of all the required components. This feature softens the knowledge barrier, enabling developers to build secure, scalable, and efficient tools.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Red Hat currently occupies the CNCF’s &lt;a href="https://all.devstats.cncf.io/d/4/company-statistics-by-repository-group?orgId=1"&gt;number-one contributor spot&lt;/a&gt;, boasting over 550,000 contributions to projects like Docker, Kubernetes, Prometheus, and other cloud technologies. Our accessible cloud-native technology helps countless organizations and individuals to build and manage scalable, flexible, robust applications in the cloud.&lt;/p&gt; &lt;h2&gt;Red Hat open source projects&lt;/h2&gt; &lt;p&gt;In addition to Developer Hub, Red Hat’s open source projects include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/products/openshift-dev-spaces/overview"&gt;Red Hat OpenShift Dev Spaces&lt;/a&gt;:&lt;/strong&gt; Built on the open Eclipse Che project, this tool provides a consistent, secure, zero-configuration integrated development environment (IDE) to write, run, and debug applications without downloading development tools locally. It allows developers with little Kubernetes knowledge to run containerized applications in-browser on OpenShift.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://www.redhat.com/en/topics/containers/what-is-podman"&gt;Podman&lt;/a&gt;&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;and&lt;strong&gt; &lt;a href="https://podman-desktop.io/"&gt;Podman Desktop&lt;/a&gt;:&lt;/strong&gt; This daemonless open source pod manager uses a command-line interface  (CLI) and the libpod library to manage and run containers running on &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; systems. Podman’s daemonless architecture makes it more accessible to users and less susceptible to root-account-level infiltration and attacks.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://docs.openshift.com/container-platform/4.8/cicd/gitops/understanding-openshift-gitops.html"&gt;OpenShift GitOps&lt;/a&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; This OpenShift add-on uses Argo as a controller, allowing development teams to implement GitOps for workflow and cluster management. Argo monitors application configurations defined in a Git repository and reports any deviation from their configured state.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines"&gt;Tekton&lt;/a&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Tekton is a Kubernetes-native framework built on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; for developing &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration and continuous delivery (CI/CD)&lt;/a&gt; systems. Tekton pipeline tasks are loosely coupled, promoting reuse across multiple projects.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We support these tools through Developer Hub, arming developers with accessible, integrated tools to build other open source downstream tools, further illustrating the impact of open source. &lt;/p&gt; &lt;h2&gt;Red Hat open source community&lt;/h2&gt; &lt;p&gt;Red Hat prioritizes its upstream communities, which collaborate under industry standards to create enterprise-level software solutions. Noteworthy communities include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://janus-idp.io/"&gt;Janus&lt;/a&gt;&lt;/strong&gt;: Janus provides access to developer portals, plug-ins, and service catalogs, helping software engineers create functional, reliable, and flexible applications.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://www.redhat.com/en/about/open-studio"&gt;Open Studio&lt;/a&gt;&lt;/strong&gt;:&lt;strong&gt; &lt;/strong&gt;This community includes a collection of strategists, developers, writers, designers, animators, and audio producers. All work within and outside of Red Hat, and all are committed to amplifying Red Hat’s principles and creative voice. Open Studio recognizes the value of storytelling in relating the impact of the open source model. It spotlights real user stories, organizes educational and vocational events, and documents every step of its creative process using open source technologies.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;&lt;a href="https://github.com/ceph/ceph"&gt;Ceph&lt;/a&gt;&lt;/strong&gt;: This upstream community supports Red Hat Data Services like Red Hat Ceph Storage and Red Hat OpenShift Data Foundation, benefitting from the collaborative efforts of its contributors. These communities create a diverse ecosystem that fosters skill development and provides access to networking opportunities through mentoring and event sponsorships.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Developers can contribute to community projects’ source codes by sharing bug fixes, updates, feature enhancements, and documentation. Additionally, communities like Janus produce industry-standard tools, providing developers with low-cost, secure, and integrated solutions that expedite development timelines. &lt;/p&gt; &lt;h2&gt;The power of open source&lt;/h2&gt; &lt;p&gt;Red Hat believes in the power of open source and the communities that drive the movement. Our open development model relies on and facilitates collaboration for building and maintaining software, so developers can get involved and help solidify the future of open source as a technology mainstay. &lt;/p&gt; &lt;p&gt;Are you looking to contribute and build your skills? Join the Red Hat open source community today by signing up for the &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/developer-program"&gt;Red Hat Developer program&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/21/red-hat-community-commitment-open-source" title="The Red Hat community commitment to open source"&gt;The Red Hat community commitment to open source&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mithun T. Dhar</dc:creator><dc:date>2023-09-21T13:00:00Z</dc:date></entry><entry><title>Network testing with testpmd and noisy_vnf</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/21/network-testing-testpmd-and-noisyvnf" /><author><name>Mike Pattrick</name></author><id>f3a11ccf-dc3e-4f1e-b869-305dee44ed80</id><updated>2023-09-21T07:00:00Z</updated><published>2023-09-21T07:00:00Z</published><summary type="html">&lt;p&gt;Developers of software-defined network (SDN) frameworks and applications often use the &lt;a href="dpdk.org"&gt;DPDK&lt;/a&gt; utility &lt;a href="https://doc.dpdk.org/guides/testpmd_app_ug/"&gt;testpmd&lt;/a&gt; to test DPDK features and benchmark network hardware performance. In the upcoming DPDK release &lt;a href="http://doc.dpdk.org/guides/rel_notes/release_23_07.html"&gt;23.07&lt;/a&gt;, we've extended the functionality of the &lt;code&gt;noisy_vnf&lt;/code&gt; module in &lt;code&gt;testpmd&lt;/code&gt; to allow better simulations of &lt;a href="https://www.redhat.com/en/blog/openshift-enterprise-production"&gt;heavily loaded servers&lt;/a&gt; or &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes"&gt;complex workloads&lt;/a&gt;. This can allow for more realistic &lt;a href="https://access.redhat.com/articles/6969629"&gt;benchmarks&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The default DPDK forwarding mode is &lt;code&gt;io&lt;/code&gt;, which reads a batch of packets from an Rx queue and writes those packets directly to a Tx queue. Because the packets aren't processed or modified, the throughput achieved with this forwarding mode could be considered a theoretical maximum and not likely to be reproduced by real-world applications. Other forwarding modes like &lt;code&gt;macswap&lt;/code&gt; and &lt;code&gt;5tswap&lt;/code&gt; process and modify layer 2 and 3, respectively; however, they do not perform any other per packet or batch processing. These forwarding modes are described in more detail in the DPDK &lt;a href="https://doc.dpdk.org/guides/testpmd_app_ug/testpmd_funcs.html#set-fwd"&gt;testpmd documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;noisy_vnf&lt;/code&gt; module has existed in DPDK since 18.11, but its usefulness has been limited by the inability to combine its use with other forwarding modes like &lt;code&gt;macswap&lt;/code&gt; or &lt;code&gt;5tswap&lt;/code&gt;. Instead, it has only forwarded packets unmodified like the io module does. This has hindered its inclusion in complex simulations. With DPDK 23.07, &lt;code&gt;noisy_vnf&lt;/code&gt; users will be able to select various forwarding modes, including &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;macswap&lt;/code&gt;, and &lt;code&gt;5tswap&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Network configuration&lt;/h2&gt; &lt;p&gt;To illustrate how this can be used, I've configured a small network with a simple application that sends and receives UDP packets; this application measures the round trip time (RTT) between send and receive by embedding a timestamp in the packet on send and comparing that timestamp to the current time receipt. The network path takes traffic out of a physical network interface into a second interface through &lt;a href="http://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt;, and finally, to &lt;code&gt;testpmd&lt;/code&gt;. See Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/noisy.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/noisy.png?itok=7Xt9-f3a" width="600" height="656" alt="Network topology" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Network topology&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The use of Open vSwitch could allow for a variety of overlay networks to quickly be swapped in or out. For example, a VLAN or &lt;a href="https://www.redhat.com/en/blog/what-geneve"&gt;GENEVE&lt;/a&gt; VPN could easily be configured. But in this example, the default &lt;a href="https://docs.openvswitch.org/en/latest/ref/ovs-actions.7/#the-ovs-normal-pipeline"&gt;Normal&lt;/a&gt; action is left in place; this causes Open vSwitch to act like a traditional switch.&lt;/p&gt; &lt;p&gt;The client application was configured to write a timestamp into the packet that it sent and then compare that with the current time when received to get an accurate measure of end-to-end RTT.&lt;/p&gt; &lt;h2&gt;Software configuration&lt;/h2&gt; &lt;p&gt;We ran three separate tests to show how &lt;code&gt;noisy_vnf&lt;/code&gt; parameters, as described below, can affect transmission rates.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;noisy-forward-mode&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; The &lt;code&gt;noisy_vnf&lt;/code&gt; forwarding mode; currently only &lt;code&gt;io&lt;/code&gt;, &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;macswap&lt;/code&gt;, and &lt;code&gt;5tswap&lt;/code&gt; are supported.&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-tx-sw-buffer-size&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; Allocates a FIFO packet queue in number of packets. Ingress packets fill this buffer until it is full or until a flush time has expired.&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-tx-sw-buffer-flushtime&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; Flush time, in milliseconds, for the FIFO packet buffer&lt;/li&gt; &lt;li&gt;&lt;code&gt;noisy-lkup-memory&lt;/code&gt;&lt;strong&gt;:&lt;/strong&gt; The amount of memory to allocate – in MB – for random read/write activity.&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;noisy-lkup-num-reads&lt;/code&gt;, &lt;code&gt;noisy-lkup-num-writes&lt;/code&gt;, &lt;code&gt;noisy-lkup-num-reads-writes&lt;/code&gt;: The number of reads and/or writes to conduct in the memory buffer allocated from &lt;code&gt;noisy-lkup-memory&lt;/code&gt; per packet.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The first test only used the &lt;code&gt;5tswap&lt;/code&gt; forwarding mode. Next, &lt;code&gt;noisy_vnf&lt;/code&gt; was configured to perform 200 random reads and 20 random writes across 400 MB of memory. Finally, &lt;code&gt;noisy_vnf&lt;/code&gt; was configured to perform 32 random reads and 8 random writes across a 200 MB buffer and to buffer 128 packets in a FIFO pipeline. The actual &lt;code&gt;testpmd&lt;/code&gt; command-line invocations are included below.&lt;/p&gt; &lt;p&gt;The first set of &lt;code&gt;noisy_vnf&lt;/code&gt; parameters attempts to simulate an application that requires a lot of processing per packet, including accessing memory that may have fallen out of the CPU cache; for example, traversing a large data structure like nested hash tables. The second set of parameters attempts to simulate an application that has to queue up multiple packets before processing them all at once. The per-packet read and write rates are much lower but, in aggregate, add up to a larger amount of memory activity per batch of packets.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# 5tswap run dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --rxq 4 --forward-mode=5tswap&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# noisy_vnf run 1 dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --noisy-tx-sw-buffer-size=0 --noisy-tx-sw-buffer-flushtime=0 \ --noisy-lkup-memory=400 --noisy-lkup-num-writes=20 --noisy-lkup-num-reads=200 \ --noisy-forward-mode=5tswap --rxq 4 --forward-mode=noisy&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# noisy_vnf run 2 dpdk-testpmd --in-memory --single-file-segment --no-pci --vdev \ 'net_virtio_user0,mac=00:01:02:03:04:05,path=/tmp/vhost0,server=1,queues=4' \ -- -i --noisy-tx-sw-buffer-size=128 --noisy-tx-sw-buffer-flushtime=20 \ --noisy-lkup-memory=200 --noisy-lkup-num-writes=8 --noisy-lkup-num-reads=32 \ --noisy-forward-mode=5tswap --rxq 4 --forward-mode=noisy&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# OVS configurations 34a5a4cc-69e2-46e5-abed-8a73e5b08dd7 Bridge noisytest datapath_type: netdev Port dpdk0 Interface dpdk0 type: dpdkvhostuserclient options: {n_rxq="4", vhost-server-path="/tmp/vhost0"} Port noisytest Interface noisytest type: internal Port dpdk1 Interface dpdk1 type: dpdk options: {dpdk-devargs="0000:03:00.1", n_rxq="4", n_rxq_desc="4096", n_txq_desc="4096"} ovs_version: "3.0.90"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The client application used for this test is available on &lt;a href="https://github.com/mkp-rh/pitcher"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;p&gt;The table below shows performance results in packets per second and microsecond round trip time while only changing the &lt;code&gt;testpmd&lt;/code&gt; invocation.&lt;/p&gt; &lt;h3&gt;5tswap test&lt;/h3&gt; &lt;p&gt;Packet source and destination addresses swapped.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second:&lt;/strong&gt; 145,000&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time:&lt;/strong&gt; 134&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;noisy_vnf 1 test&lt;/h3&gt; &lt;p&gt;Source and destination swapped; 220 memory operations over 400 MB of memory per batch.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second: &lt;/strong&gt;40,000&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time:&lt;/strong&gt; 235&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;noisy_vnf 2 test&lt;/h3&gt; &lt;p&gt;Source and destination swapped; 40 memory operations over 200 MB of memory per batch; and 128 packets queued in a FIFO buffer.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Packets per second: &lt;/strong&gt;992&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Round-trip time: &lt;/strong&gt;20,225&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;These results demonstrate how substantially a packet buffer or a lot of random memory access can impact packet throughput. Merely inserting a few hundred memory operations with each packet saw a near doubling of round trip time. Using a FIFO packet buffer had a 40x reduction in packets per second throughput. These results make it clear that a network speed test that doesn't involve any per packet processing will not give a complete picture of the performance capabilities of the network setup.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/21/network-testing-testpmd-and-noisyvnf" title="Network testing with testpmd and noisy_vnf"&gt;Network testing with testpmd and noisy_vnf&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mike Pattrick</dc:creator><dc:date>2023-09-21T07:00:00Z</dc:date></entry><entry><title>Automate your AMQ streams platform with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/20/automate-your-amq-streams-platform-ansible" /><author><name>Roman Martin Gil</name></author><id>d56fa280-2f57-4d50-9460-febc163988e3</id><updated>2023-09-20T07:00:00Z</updated><published>2023-09-20T07:00:00Z</published><summary type="html">&lt;p&gt;Nowadays, &lt;a href="http://developers.redhat.com/topics/automation"&gt;IT automation&lt;/a&gt; is a must to accelerate, improve and deliver value in a secured, tested, and easy way. &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; has become the go-to tool for IT teams due to its simplicity, versatility, and powerful automation capabilities. With its agentless architecture, Ansible allows for seamless deployment, configuration management, and orchestration across a wide range of systems and platforms.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kuberneteshttp://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; has emerged as the preferred tool for handling real-time data streams. Its distributed architecture, fault tolerance, and scalability make it a robust choice for building &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven architecture (EDA)&lt;/a&gt;. &lt;a href="https://access.redhat.com/products/red-hat-amq-streams/"&gt;Red Hat AMQ streams&lt;/a&gt; is the enterprise edition of this platform provided by Red Hat. &lt;/p&gt; &lt;p&gt;Both amazing tools meet together in the &lt;a href="https://github.com/ansible-middleware"&gt;Ansible Middleware&lt;/a&gt; community, providing the &lt;a href="https://galaxy.ansible.com/middleware_automation/amq_streams"&gt;Ansible collection for Red Hat AMQ streams&lt;/a&gt;. This collection includes a set of automation capabilities and features to manage and operate Apache Kafka clusters on top of &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to easily deploy an Apache Kafka cluster based on AMQ streams using the Ansible collection for Red Hat AMQ streams.&lt;/p&gt; &lt;h2&gt;Install the collection&lt;/h2&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To make use of this tutorial, you need a Red Hat Enterprise Linux or Fedora system, along with version 2.12 or higher of Ansible (preferably the latest version).&lt;/p&gt; &lt;p&gt;The very first step, of course, is to install the collection itself so that Ansible can use its content inside playbooks:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.amq_streams&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before going further, you should check to make sure that the collection has been successfully installed. To do so, run the following command from Ansible Galaxy that will list all the installed collections:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection list  Collection                          Version ----------------------------------- ------- ansible.posix 1.5.4 middleware_automation.amq_streams   0.0.5   &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; As this collection is under development and evolution, the version downloaded might differ from the version displayed.&lt;/p&gt; &lt;p&gt;Now that you've installed the collection and its dependencies, you can use them to automate the installation of AMQ streams.&lt;/p&gt; &lt;h2&gt;Deploy AMQ streams with Ansible&lt;/h2&gt; &lt;p&gt;Thanks to the dedicated Ansible collection for AMQ streams, automating the installation and configuration of Apache Kafka is easy. However, before you implement this collection inside your playbook, we should recap what we mean here by &lt;strong&gt;installing&lt;/strong&gt; and &lt;strong&gt;configuring&lt;/strong&gt; Apache Kafka. Indeed, this task encompasses quite a few operations that are performed on the target system:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creating appropriate OS user and group accounts.&lt;/li&gt; &lt;li aria-level="1"&gt;Downloading the installation archive from the Kafka website.&lt;/li&gt; &lt;li aria-level="1"&gt;Unarchiving the contents while ensuring that all the files are associated with the appropriate user and groups along with the correct permissions.&lt;/li&gt; &lt;li aria-level="1"&gt;Ensuring that the required version of the &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; Virtual Machine (JVM) is installed.&lt;/li&gt; &lt;li aria-level="1"&gt;Integrating the software into the host service management service— in our case, the Linux &lt;a href="https://www.linux.com/training-tutorials/understanding-and-using-systemd/"&gt;systemd&lt;/a&gt; daemon.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Apache Kafka is a distributed ecosystem of different components with multiple deployment options and capabilities. The Ansible collection for AMQ streams helps automate most common deployment and configuration topologies. This article shows one of these cases for the purposes of demonstrating the capabilities of the collection. For further information, please refer to the &lt;a href="https://ansiblemiddleware.com/amq_streams/"&gt;Ansible collection for AMQ streams documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates our use case: a distributed cluster formed by a ZooKeeper ensemble of three nodes and three Kafka brokers managed by Ansible.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/amq-streams-ansible.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/amq-streams-ansible.png?itok=Zk08NuOI" width="600" height="427" alt="AMQ Streams deployment managed by Ansible" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: AMQ Streams deployment managed by Ansible&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;All of this is achieved and is fully automated by the following playbook, combining several different roles included within the Ansible collection of AMQ streams.&lt;/p&gt; &lt;p&gt;Create a playbook at the path &lt;code&gt;playbooks/my-amq_streams_distributed.yml&lt;/code&gt; containing the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Ansible Playbook to install a Zookeeper ensemble and Kafka Broker Authenticated" hosts: all vars: # Enabling Zookeeper Authentication amq_streams_zookeeper_auth_enabled: true amq_streams_zookeeper_auth_user: zkadmin amq_streams_zookeeper_auth_pass: p@ssw0rd # Enabling Kafka BrokerListeners amq_streams_broker_listeners: - AUTHENTICATED://:{{ amq_streams_broker_listener_port }} # Authenticated - REPLICATION://:{{ amq_streams_broker_listener_internal_port }} # Inter broker communication # Listener for inter-broker communications amq_streams_broker_inter_broker_listener: REPLICATION # Enabling Kafka Broker Authentication amq_streams_broker_auth_enabled: true amq_streams_broker_auth_scram_enabled: true amq_streams_broker_auth_listeners: - AUTHENTICATED:SASL_PLAINTEXT - REPLICATION:SASL_PLAINTEXT amq_streams_broker_auth_sasl_mechanisms: - PLAIN - SCRAM-SHA-512 # Kafka Plain Users amq_streams_broker_auth_plain_users: - username: admin password: p@ssw0rd - username: kafkauser password: p@ssw0rd # Setting Kafka user for inter-broker communication amq_streams_broker_inter_broker_auth_sasl_mechanisms: PLAIN amq_streams_broker_inter_broker_auth_broker_username: interbroker amq_streams_broker_inter_broker_auth_broker_password: p@ssw0rd # Enabling Broker replication amq_streams_broker_offsets_topic_replication_factor: 3 amq_streams_broker_transaction_state_log_replication_factor: 3 amq_streams_broker_transaction_state_log_min_isr: 2 roles: - role: amq_streams_zookeeper tasks: - name: "Ensure Zookeeper is running and available." ansible.builtin.include_role: name: amq_streams_zookeeper - name: "Ensure AMQ Streams Broker is running and available." ansible.builtin.include_role: name: amq_streams_broker post_tasks: - name: "Display numbers of Zookeeper instances managed by Ansible." ansible.builtin.debug: msg: "Numbers of Zookeeper instances: {{ amq_streams_zookeeper_instance_count }}." when: - amq_streams_zookeeper_instance_count_enabled is defined and amq_streams_zookeeper_instance_count_enabled - name: "Display numbers of broker instances managed by Ansible." ansible.builtin.debug: msg: "Numbers of broker instances: {{ amq_streams_broker_instance_count }}." when: - amq_streams_broker_instance_count_enabled is defined and amq_streams_broker_instance_count_enabled - name: "Validate that Zookeeper deployment is functional." ansible.builtin.include_role: name: amq_streams_zookeeper tasks_from: validate.yml - name: "Validate that Broker deployment is functional." ansible.builtin.include_role: name: amq_streams_broker tasks_from: validate.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The playbook includes a set of different users and their credentials. Note that because these variables contain sensitive data, they should be secured using &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;Ansible Vault&lt;/a&gt; or some other secrets management system. However, that task is beyond the scope of this article.&lt;/p&gt; &lt;p&gt;The inventory of hosts to deploy the desired topology looks similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;[all] rhel9mw01 rhel9mw02 rhel9mw03 [zookeepers] rhel9mw01 rhel9mw02 rhel9mw03 [brokers] rhel9mw01 rhel9mw02 rhel9mw03 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a new file called &lt;code&gt;inventory&lt;/code&gt; with the contents above.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;zookeepers&lt;/code&gt; group identifies the hosts to deploy the ZooKeeper component, and the &lt;code&gt;brokers&lt;/code&gt; group identifies the hosts to deploy the Kafka brokers.&lt;/p&gt; &lt;p&gt;Run this playbook as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory playbooks/my-amq_streams_distributed.yml&lt;/code&gt;&lt;/pre&gt; &lt;div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In order for this playbook to perform the installation outlined here, Ansible must have &lt;code&gt;sudo&lt;/code&gt; or root privileges on the target hosts.&lt;/p&gt; &lt;h2&gt;Check for successful installation&lt;/h2&gt; &lt;p&gt;Once the playbook finishes its execution, you can confirm that the ZooKeeper and Broker services are now running by verifying their status.&lt;/p&gt; &lt;h3&gt;ZooKeeper&lt;/h3&gt; &lt;p&gt;Check the ZooKeeper service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[rhmw@rhel9mw01 logs]$ sudo systemctl status amq_streams_zookeeper.service ● amq_streams_zookeeper.service - amq_streams_zookeeper Loaded: loaded (/usr/lib/systemd/system/amq_streams_zookeeper.service; enabled; preset: disabled) Drop-In: /usr/lib/systemd/system/service.d └─10-timeout-abort.conf Active: active (running) since Wed 2023-06-28 15:16:03 CEST; 3min 59s ago Main PID: 4873 (java) Tasks: 52 (limit: 2298) Memory: 125.1M CPU: 3.567s CGroup: /system.slice/amq_streams_zookeeper.service └─4873 java -Xmx256M -Xms256M -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true "-Xlog&gt; Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,050] INFO Committing global session 0x300000721bc0000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,320] INFO Committing global session 0x10000064a600000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,390] INFO Successfully authenticated client: authenticationID=zkadmin; authorizationID=zkadmin. (org.apache.zookeeper.server.auth.Sas&gt; Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,455] INFO Setting authorizedID: zkadmin (org.apache.zookeeper.server.auth.SaslServerCallbackHandler) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,458] INFO adding SASL authorization for authorizationID: zkadmin (org.apache.zookeeper.server.ZooKeeperServer) Jun 28 15:16:36 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:36,543] INFO Committing global session 0x300000721bc0001 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:48 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:48,086] INFO Submitting global closeSession request for session 0x10000064a600000 (org.apache.zookeeper.server.ZooKeeperServer) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,524] INFO Committing global session 0x300000721bc0002 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,650] INFO Committing global session 0x200000647ab0000 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) Jun 28 15:16:50 rhel9mw01 zookeeper-server-start.sh[4873]: [2023-06-28 15:16:50,816] INFO Committing global session 0x300000721bc0003 (org.apache.zookeeper.server.quorum.LearnerSessionTracker) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Kafka broker&lt;/h3&gt; &lt;p&gt;Check the Kafka broker service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;rhmw@rhel9mw01 logs]$ sudo systemctl status amq_streams_broker.service ● amq_streams_broker.service - amq_streams_broker Loaded: loaded (/usr/lib/systemd/system/amq_streams_broker.service; enabled; preset: disabled) Drop-In: /usr/lib/systemd/system/service.d └─10-timeout-abort.conf Active: active (running) since Wed 2023-06-28 15:16:48 CEST; 5min ago Main PID: 11241 (java) Tasks: 78 (limit: 2298) Memory: 319.1M CPU: 8.601s CGroup: /system.slice/amq_streams_broker.service └─11241 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true "-Xlog:gc&gt; Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,104] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,208] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,240] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,251] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka version: 3.3.2 (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka commitId: b66af662e61082cb (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,257] INFO Kafka startTimeMs: 1687958212256 (org.apache.kafka.common.utils.AppInfoParser) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,258] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,335] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use node rhel9mw01:9091 (i&gt; Jun 28 15:16:52 rhel9mw01 kafka-server-start.sh[11241]: [2023-06-28 15:16:52,383] INFO [BrokerToControllerChannelManager broker=0 name=alterPartition]: Recorded new controller, from now on will use node rhel9mw01:909 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the Kafka cluster&lt;/h2&gt; &lt;p&gt;So, our Apache Kafka cluster is up and running and ready to go and manage streaming events. Let's test it by creating a simple topic, producing some messages, and consuming them.&lt;/p&gt; &lt;p&gt;This test can be accomplished using the Kafka CLI scripts below for these actions. These commands are simple examples and can be extended to implement more complex actions in your environment.&lt;/p&gt; &lt;p&gt;As our Kafka cluster requires authentication, we need to create a file with the user credentials and authentication mechanism to reference when invoking the CLI. In our case, this file will use the &lt;code&gt;kafkauser&lt;/code&gt; user created by the playbook.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cat &lt;&lt;EOF &gt; /tmp/kafka-cli.properties security.protocol=SASL_PLAINTEXT sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="kafkauser" password="password"; EOF &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will create the &lt;code&gt;sample-topic&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --command-config /tmp/kafka-cli.properties --create --topic sample-topic --partitions 10 --replication-factor 3 Created topic sample-topic. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;kafka-console-producer.sh&lt;/code&gt; script allows for sending messages to a topic. The following actions will publish messages to the topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic sample-topic --producer.config /tmp/kafka-cli.properties &gt;Hello Ansible Collection for AMQ Streams!!!! &gt;Another message!! &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With a series of messages published, the &lt;code&gt;kafka-console-consumer.sh&lt;/code&gt; script can be used to consume the messages. The following command will enable the consumption of the messages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic sample-topic --consumer.config /tmp/kafka-cli.properties --from-beginning --timeout-ms 10000 Hello Ansible Collection for AMQ Streams!!!! Another message!! [2023-06-28 16:04:24,896] ERROR Error processing message, terminating consumer process: (kafka.tools.ConsoleConsumer$) org.apache.kafka.common.errors.TimeoutException Processed a total of 2 messages &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cool! Your Apache Kafka cluster is ready and fully automated by Ansible.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;By using Ansible and the Ansible collection for AMQ streams as outlined in this article, you can fully automate the deployment of an event streaming platform in any RHEL-based environment without any manual intervention. Ansible performed all the heavy lifting (downloading software, preparing the OS, creating users and groups, deploying the binary files and the configuration, setting up the service, and more) and even setting up the cluster with a set of users.&lt;/p&gt; &lt;p&gt;The Ansible collection for AMQ streams allows you to streamline the installation and configuration of Apache Kafka, thus enabling you to scale deployments as necessary and ensure repeatability across them all.&lt;/p&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/20/automate-your-amq-streams-platform-ansible" title="Automate your AMQ streams platform with Ansible"&gt;Automate your AMQ streams platform with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Roman Martin Gil</dc:creator><dc:date>2023-09-20T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.4.1 released - Redis 7.2 and Flyway changes</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-4-1-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-4-1-released/</id><updated>2023-09-20T00:00:00Z</updated><published>2023-09-20T00:00:00Z</published><summary type="html">It is our pleasure to announce the release of Quarkus 3.4.1. We skipped 3.4.0 as we needed a fix for CVE-2023-4853 in 3.4 too. Major changes are: Support for Redis 7.2 Adjustments on how to enable/activate Flyway This version also comes with bugfixes, performance improvements and documentation improvements. We currently...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-20T00:00:00Z</dc:date></entry><entry><title>Write operators in Java with JOSDK, Part 4: Upgrading strategies</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/19/write-operators-java-josdk-part-4-upgrading-strategies" /><author><name>Christophe Laprun</name></author><id>5a7e37cf-e472-4465-8b2f-4226bbc93d45</id><updated>2023-09-19T07:00:00Z</updated><published>2023-09-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt;(JOSDK) is an open source project that aims to simplify the task of creating Kubernetes operators using Java. &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt; started the project, and Red Hat is now a major contributor. The JOSDK project now lives under the &lt;a href="https://github.com/operator-framework"&gt;Operator Framework umbrella&lt;/a&gt;, which is a &lt;a href="https://cncf.io"&gt;Cloud Native Computing Foundation (CNCF)&lt;/a&gt; incubating project.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk"&gt;first article in this series&lt;/a&gt; introduced JOSDK and explained why it could be interesting to create operators in Java. The &lt;a href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2"&gt;second article&lt;/a&gt; showed how the &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;JOSDK Quarkus extension &lt;code&gt;quarkus-operator-sdk&lt;/code&gt;&lt;/a&gt;, also called QOSDK, facilitates the development experience by taking care of managing the Custom Resource Definition automatically. The &lt;a href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller"&gt;third article&lt;/a&gt; focused on requirements for implementing the reconciliation logic for the example operator you build in this series. Many things have changed since the third installment of this series. This article will thus focus on updating the code to the latest versions and provide upgrading strategies.&lt;/p&gt; &lt;h2&gt;Where things stand&lt;/h2&gt; &lt;p&gt;You implemented a simple operator exposing your application outside the cluster via an &lt;code&gt;Ingress&lt;/code&gt;, creating the associated &lt;code&gt;Deployment&lt;/code&gt; and &lt;code&gt;Service&lt;/code&gt; along the way. However, it has been a while since the last part of this blog series and many things have changed. When the third article was written, QOSDK was in version 3.0.4. Now it is up to 6.3.0. Quarkus has also been updated. How can you update your operator to use more recent versions, and what are possible strategies to update your code?&lt;/p&gt; &lt;h2&gt;How to use Quarkus update&lt;/h2&gt; &lt;p&gt;Upgrading a project is always a tricky proposition, especially when there is a wide gap between the old and new versions. Quarkus can help you with this task, though it might not work in all cases. In this case, you want to migrate from Quarkus 2.7.3.Final to the latest version, which at the time of writing this article, is 3.2.4.Final. You can use the &lt;code&gt;update&lt;/code&gt; command that Quarkus provides. If you have the &lt;code&gt;quarkus&lt;/code&gt; command line tool, you might want to upgrade it first and then simply run &lt;code&gt;quarkus update&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Otherwise, using maven only, you can run:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;mvn io.quarkus.platform:quarkus-maven-plugin:3.2.4.Final:update -N&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The complete procedure is detailed in the &lt;a href="https://quarkus.io/guides/update-quarkus"&gt;related Quarkus guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In your case, you should notice that the update procedure fails with an error when the command attempts to check the updated project as follows:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;[INFO] [ERROR] [ERROR] Some problems were encountered while processing the POMs: [INFO] [ERROR] 'dependencies.dependency.version' for io.quarkiverse.operatorsdk:quarkus-operator-sdk-csv-generator:jar is missing. @ line 38, column 17&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Updating outdated QOSDK dependency&lt;/h2&gt; &lt;p&gt;The problem occurs because this dependency doesn’t exist anymore. Though the project actually doesn’t need this dependency at this point, it is included by default when bootstrapping a QOSDK project using the &lt;code&gt;operator-sdk&lt;/code&gt; CLI and allows for automatic generation of &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt; bundles. OLM enables you to manage the lifecycle of operators on clusters in a more principled way. We might discuss this feature in greater detail in a future article.&lt;/p&gt; &lt;p&gt;There are two ways to fix your project. If you’re not interested in the feature, you can remove the dependency, or change it to the correct one. This dependency doesn’t exist in its previous form anymore because it has been renamed to better reflect its expanded scope. It initially focused solely on the &lt;a href="https://olm.operatorframework.io/docs/concepts/crds/clusterserviceversion/"&gt;&lt;code&gt;ClusterServiceVersion&lt;/code&gt;&lt;/a&gt; part of OLM bundles, but now extends to generating complete bundles. The feature was actually disabled using &lt;code&gt;quarkus.operator-sdk.generate-csv=false&lt;/code&gt; in the &lt;code&gt;application.properties&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;The new dependency name is &lt;code&gt;quarkus-operator-sdk-bundle-generator&lt;/code&gt;. So use that if you want to use the OLM generation feature. Note that you will also need to change the associated property name to activate the feature. You’ll see a warning in the logs that the property doesn’t exist if you don’t, and the OLM generation will be activated by default. The new property is named &lt;code&gt;quarkus.operator-sdk.bundle.enabled&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After making these changes, you can re-run the update command. It should now succeed, with an output similar to the following:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="shell"&gt;[INFO] Detected project Java version: 11 [INFO] Quarkus platform BOMs: [INFO] io.quarkus:quarkus-bom:pom:3.2.4.Final ✔ [INFO] Add: io.quarkus.platform:quarkus-operator-sdk-bom:pom:3.2.4.Final [INFO] [INFO] Extensions from io.quarkus:quarkus-bom: [INFO] io.quarkus:quarkus-micrometer-registry-prometheus ✔ [INFO] [INFO] Extensions from io.quarkus.platform:quarkus-operator-sdk-bom: [INFO] Update: io.quarkiverse.operatorsdk:quarkus-operator-sdk-bundle-generator:6.3.0 -&gt; remove version (managed) [INFO] Update: io.quarkiverse.operatorsdk:quarkus-operator-sdk:6.3.0 -&gt; remove version (managed)&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Strategies for QOSDK and Quarkus updates&lt;/h2&gt; &lt;p&gt;Now you can see that you can actually simplify things even further. It is advising you to add the &lt;code&gt;io. quarkus.platform:quarkus-operator-sdk-bom:pom:3.2.4.Final&lt;/code&gt; dependency. Indeed, QOSDK has been added to the Quarkus platform, making it easier to consume from a given Quarkus version. Switching to this BOM only allows you to decide which version of Quarkus to use, and the BOM will make sure you get the appropriate QOSDK version.&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="xml"&gt;&lt;strong&gt;&lt;dependencyManagement&gt;&lt;/strong&gt; &lt;strong&gt;&lt;dependencies&gt;&lt;/strong&gt; &lt;strong&gt;&lt;dependency&gt;&lt;/strong&gt; &lt;strong&gt;&lt;groupId&gt;&lt;/strong&gt;io.quarkiverse.operatorsdk&lt;strong&gt;&lt;/groupId&gt;&lt;/strong&gt; &lt;strong&gt;&lt;artifactId&gt;&lt;/strong&gt;quarkus-operator-sdk-bom&lt;strong&gt;&lt;/artifactId&gt;&lt;/strong&gt; &lt;strong&gt;&lt;version&gt;&lt;/strong&gt;${quarkus-sdk.version}&lt;strong&gt;&lt;/version&gt;&lt;/strong&gt; &lt;strong&gt;&lt;scope&gt;&lt;/strong&gt;import&lt;strong&gt;&lt;/scope&gt;&lt;/strong&gt; &lt;strong&gt;&lt;type&gt;&lt;/strong&gt;pom&lt;strong&gt;&lt;/type&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependency&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependencies&gt;&lt;/strong&gt; &lt;strong&gt;&lt;/dependencyManagement&gt;&lt;/strong&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This project is currently using the QOSDK BOM with &lt;code&gt;quarkus-sdk.version&lt;/code&gt; with the 3.0.4 value. You’ll also note that there is a &lt;code&gt;quarkus.version&lt;/code&gt; property with the 2.7.3.Final value. Looking at the QOSDK BOM, you can see that there is also a Quarkus version property defined there, with the same &lt;code&gt;quarkus.version&lt;/code&gt; name. Therefore, if you upgrade the QOSDK version, with the current setup, you need to make sure to also upgrade the Quarkus version in your project in such a way that is compatible with the version defined in the QOSDK BOM.&lt;/p&gt; &lt;p&gt;Using the QOSDK BOM defined by the Quarkus platform (i.e., Using the &lt;code&gt;io.quarkus.platform:quarkus-operator-sdk-bom&lt;/code&gt; artifact instead of the &lt;code&gt;io.quarkiverse.operatorsdk:quarkus-operator-sdk-bom&lt;/code&gt;, note the different group identifier.) simplifies this aspect by making sure that both QOSDK and Quarkus versions are aligned. The downside of this is that using the QOSDK BOM directly from the QOSDK project, you have the Quarkus BOM automatically included in your project. The price for this, as previously explained, is that you need to make sure the versions are in sync.&lt;/p&gt; &lt;p&gt;That said, you can also see that it is letting us know that there is a more recent version of the QOSDK extension (6. 3.0), which will only be available from the Quarkus platform starting with version 3.2.5.Final. Using the Quarkus platform, therefore, means that you’re not necessarily using the latest QOSDK version. This is, however, the version that is verified to work with the platform as a whole, so this is the more conservative option.&lt;/p&gt; &lt;p&gt;If you wish to use the absolute latest version of QOSDK, you should use the BOM provided by QOSDK, but you will need to make sure to update the Quarkus version using the &lt;code&gt;quarkus.version&lt;/code&gt;, while updating the QOSDK version using the &lt;code&gt;quarkus-sdk.version&lt;/code&gt; property in your &lt;code&gt;pom.xml&lt;/code&gt; file as previously done.&lt;/p&gt; &lt;p&gt;Which approach to choose depends on your appetence for risk, or how you wish to manage your dependencies. Generally speaking, the Quarkus platform is updated frequently, and QOSDK versions are usually updated accordingly as needed. So the Quarkus platform is usually up-to-date when it comes to the latest QOSDK version. If you absolutely need the latest QOSDK version, upgrading from the Quarkus platform offerings, a patch or even a minor version should typically work with issues since QOSDK strives to maintain backwards compatibility between minor versions.&lt;/p&gt; &lt;p&gt;Going the opposite direction, upgrading Quarkus to a minor version above (e.g., from 3.2.x to 3.3.x) might prove tricky since the Fabric8 Kubernetes client version used by that new Quarkus version might also have been updated to a new minor version. This has been known to bring API changes, so you might want to tread carefully with such updates.&lt;/p&gt; &lt;p&gt;Actually, QOSDK issues debug-level warnings when it detects version mismatches (minor version and above, patch level mismatches considered safe) between Quarkus, JOSDK, and Fabric8 Kubernetes client. You can even configure it to fail a build by setting the &lt;code&gt;quarkus.operator-sdk.fail-on-version-check&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;. Please refer to the &lt;a href="https://docs.quarkiverse.io/quarkus-operator-sdk/dev/index.html#quarkus-operator-sdk_quarkus.operator-sdk.fail-on-version-check"&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;It’s also worth repeating that since QOSDK bundles JOSDK, you do not need to worry about updating that dependency separately. One less thing to worry about.&lt;/p&gt; &lt;h2&gt;Adapting to Fabric8 Kubernetes client changes&lt;/h2&gt; &lt;p&gt;Now that the dependencies are sorted out, if you try to build now, you should get a compilation error due to an API change in the Fabric8 Kubernetes client:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.1:compile (&lt;strong&gt;default&lt;/strong&gt;-compile) on project expose: Compilation failure [ERROR] exposedapp-rhdblog/src/main/java/io/halkyon/ExposedAppReconciler.java:[63,33] cannot find symbol [ERROR] symbol: method withIntVal(&lt;strong&gt;int&lt;/strong&gt;) [ERROR] location: &lt;strong&gt;interface&lt;/strong&gt; &lt;strong&gt;io&lt;/strong&gt;.fabric8.kubernetes.api.model.ServicePortFluent.TargetPortNested&lt;io.fabric8.kubernetes.api.model.ServiceSpecFluent.PortsNested&lt;io.fabric8.kubernetes.api.model.ServiceFluent.SpecNested&lt;io.fabric8.kubernetes.api.model.ServiceBuilder&gt;&gt;&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This issue is easily fixed by changing the following line:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;.withNewTargetPort().withIntVal(8080).endTargetPort()&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to:&lt;/p&gt; &lt;pre class="CodeRay highlight"&gt; &lt;code data-lang="java"&gt;.withNewTargetPort(8080)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Fabric8 Kubernetes client provides detailed notes for each release. It’s always a good idea to take a look at them, especially whenever a new minor version is released (&lt;a href="https://github.com/fabric8io/kubernetes-client/releases/tag/v6.8.0"&gt;here&lt;/a&gt; are the notes for the 6.8. 0 release, which does contain breaking changes). Another interesting resource is the &lt;a href="https://github.com/fabric8io/kubernetes-client/blob/main/doc/CHEATSHEET.md"&gt;cheat sheet&lt;/a&gt;, which contains a wealth of information on how to perform a wide variety of tasks using the client.&lt;/p&gt; &lt;p&gt;That said, you should now be all set for this batch of updates!&lt;/p&gt; &lt;h2 id="_conclusion"&gt;Summary&lt;/h2&gt; &lt;p&gt;While less focused on writing operators per se, this article still covered an important part of any software development: upgrading dependencies. Your operator should be now ready for improvements, which we will tackle in the next article. We will also discuss adding status handling and how to make your operator react to events that are not targeting primary resources.&lt;/p&gt; &lt;p&gt;For reference, you can find the completed code for this part under the &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog/tree/part-4"&gt;&lt;code&gt;part-4&lt;/code&gt; tag&lt;/a&gt; of the &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog"&gt;https://github.com/halkyonio/exposedapp-rhdblog&lt;/a&gt; repository.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/19/write-operators-java-josdk-part-4-upgrading-strategies" title="Write operators in Java with JOSDK, Part 4: Upgrading strategies"&gt;Write operators in Java with JOSDK, Part 4: Upgrading strategies&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2023-09-19T07:00:00Z</dc:date></entry><entry><title>When Quarkus meets Virtual Threads</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/virtual-thread-1/&#xA;            " /><author><name>Clement Escoffier (https://twitter.com/clementplop)</name></author><id>https://quarkus.io/blog/virtual-thread-1/</id><updated>2023-09-19T00:00:00Z</updated><published>2023-09-19T00:00:00Z</published><summary type="html">Java 21 offers a new feature that will reshape the development of concurrent applications in Java. For over two years, the Quarkus team explored integrating this new feature to ease the development of distributed applications, including microservices and event-driven applications. This blog post is the first part of a series...</summary><dc:creator>Clement Escoffier (https://twitter.com/clementplop)</dc:creator><dc:date>2023-09-19T00:00:00Z</dc:date></entry><entry><title>A statistics update in Open vSwitch user space datapath</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/18/statistics-update-open-vswitch-user-space-datapath" /><author><name>David Marchand</name></author><id>31406c88-48e4-4cec-a6b6-9095aa82356d</id><updated>2023-09-18T07:00:00Z</updated><published>2023-09-18T07:00:00Z</published><summary type="html">&lt;p&gt;With the demands for higher bandwidth, came the need for scaling and processing packets on more CPU resources. In Open vSwitch (OVS) using DPDK for faster IO, this translated to using more receive and transmit queues to allow more PMD threads to process the packets. This adds some complexity to a system not easy to understand in the first place. Support or operation people still want to know how much traffic is received and how it is distributed across the CPU resources. To offer help, this article describes new statistics added for the user space datapath in OVS 2.17 and later.&lt;/p&gt; &lt;h2&gt;Per queue statistics for DPDK ports&lt;/h2&gt; &lt;p&gt;A first evolution in OVS 2.17 consisted of &lt;a href="https://github.com/openvswitch/ovs/commit/1140c87e2eb7"&gt;exposing receive and transmit queues statistics&lt;/a&gt; per DPDK physical ports in ovsdb.&lt;/p&gt; &lt;p&gt;For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface dpdk0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep packets= | grep -v '=0$' rx_packets=5553474 rx_q0_packets=3705290 rx_q1_packets=1848184 tx_broadcast_packets=220 tx_multicast_packets=488 tx_packets=39406658924 tx_q1_packets=3700644 tx_q2_packets=97 tx_q3_packets=19696490438 tx_q4_packets=19706467745&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Those per queue statistics require support from the DPDK driver backing the port.&lt;/p&gt; &lt;p&gt;A vast majority of physical (and even some virtual) NIC DPDK drivers do support those statistics. But if no statistics appear in ovsdb, you may check for support by looking for the RTE_ETH_DEV_AUTOFILL_QUEUE_XSTATS (1 &lt;&lt; 6) value in the port dev_flags bitmask through the DPDK telemetry tool (coming with the dpdk-tools rpm).&lt;/p&gt; &lt;p&gt;For example, check port 0:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# if [ $(($(echo /ethdev/info,0 | dpdk-telemetry.py -f /var/run/openvswitch/dpdk/rte | jq -r '.["/ethdev/info"]["dev_flags"]') &amp; 64)) != 0 ]; then echo per queue stats are supported; else echo per queue stats may not be implemented for this driver; fi per queue stats are supported&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Per queue statistics for vhost-user ports&lt;/h2&gt; &lt;p&gt;Getting the same level of information for vhost-user ports has required some reworking in the DPDK vhost-user libary because the library was not accounting such information.&lt;/p&gt; &lt;p&gt;&lt;br /&gt; This was enhanced by the community in the DPDK v22.07 release with this &lt;a href="https://git.dpdk.org/dpdk/commit/?id=be75dc99ea1f"&gt;change&lt;/a&gt;, and its support was merged in OVS with this &lt;a href="https://github.com/openvswitch/ovs/commit/3b29286db1c5"&gt;change&lt;/a&gt; in the 3.1 version:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface vhost0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep packets= | grep -v '=0$' rx_65_to_127_packets=2987595 rx_packets=2987595 rx_q0_good_packets=2987595 rx_q0_size_65_127_packets=2987595 tx_65_to_127_packets=14075727 tx_packets=14075727 tx_q0_good_packets=14075727 tx_q0_size_65_127_packets=14075727&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;More vhost-user statistics&lt;/h2&gt; &lt;p&gt;As a bonus of the work exposing per queue statistics, the vhost-user library started exposing other internal counters.&lt;/p&gt; &lt;p&gt;The virtio driver (e.g., the Linux kernel driver by default) plugged on a vhost-user port may require guest notifications for signaling packets delivery. Triggering those notifications impacts the processing cost of such packets, which is why keeping track of the amount of notifications is of interest.&lt;/p&gt; &lt;p&gt;Previously, OVS was exposing a coverage counter for those notifications, and until OVS 3.0, you could use the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-appctl coverage/show | grep vhost_notification vhost_notification 0.0/sec 0.000/sec 2.0283/sec total: 7302&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This coverage counter was only hinting at some vhost-user ports used by an unidentified virtual machine.&lt;/p&gt; &lt;p&gt;Starting OVS 3.1, the coverage counter has been removed in favor of per queue and per port statistics (&lt;a href="https://git.dpdk.org/dpdk/commit/?id=1ea74efd7fa4"&gt;DPDK change&lt;/a&gt; / &lt;a href="https://github.com/openvswitch/ovs/commit/c9e10ac57fb8"&gt;OVS change&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ovs-vsctl get interface vhost0 statistics | sed -e 's#[{}]##g' -e 's#, #\n#g' | grep guest_notifications rx_q0_guest_notifications=12 rx_q1_guest_notifications=1 tx_q0_guest_notifications=3 tx_q1_guest_notifications=2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This nice addition makes it possible to directly point at which virtual machine is slowing down packet processing. Other vhost-user statistics have been added, like exposing the &lt;a href="https://git.dpdk.org/dpdk/commit/?id=7247b7464ef9"&gt;vhost-user IOTLB cache internals&lt;/a&gt;. More may be added in the future as members of the community express new requirements.&lt;/p&gt; &lt;h2&gt;A final note about statistics&lt;/h2&gt; &lt;p&gt;As OVS stores per interface statistics in its ovsdb, choices were made to select generic (iow not driver specific) statistics, and that helps in a majority of use cases.&lt;/p&gt; &lt;p&gt;However, if you do not find the driver-specific statistics you're looking for, it is still possible for debugging to use the DPDK telemetry tool and retrieve all unfiltered port statistics as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# echo /ethdev/xstats,0 | dpdk-telemetry.py -f /var/run/openvswitch/dpdk/rte { "/ethdev/xstats": { "rx_good_packets": 5553474, "tx_good_packets": 39406658860, "rx_good_bytes": 710844672, "tx_good_bytes": 4886425719104, "rx_missed_errors": 78892319, "rx_errors": 0, "tx_errors": 0, "rx_mbuf_allocation_errors": 0, "rx_q0_packets": 3705290, "rx_q0_bytes": 474277120, "rx_q0_errors": 0, "rx_q1_packets": 1848184, "rx_q1_bytes": 236567552, "rx_q1_errors": 0, "tx_q0_packets": 0, "tx_q0_bytes": 0, "tx_q1_packets": 3700615, "tx_q1_bytes": 458874621, "tx_q2_packets": 71, "tx_q2_bytes": 8120, "tx_q3_packets": 19696490435, "tx_q3_bytes": 2442364825811, "tx_q4_packets": 19706467739, "tx_q4_bytes": 2443602010552, "rx_wqe_errors": 0, "rx_unicast_packets": 84445793, "rx_unicast_bytes": 10471278332, "tx_unicast_packets": 39406658216, "tx_unicast_bytes": 4886425618784, "rx_multicast_packets": 0, "rx_multicast_bytes": 0, "tx_multicast_packets": 444, "tx_multicast_bytes": 35320, "rx_broadcast_packets": 0, "rx_broadcast_bytes": 0, "tx_broadcast_packets": 200, "tx_broadcast_bytes": 65000, "tx_phy_packets": 39406658860, "rx_phy_packets": 84445793, "rx_phy_crc_errors": 0, "tx_phy_bytes": 5044052354544, "rx_phy_bytes": 10809061504, "rx_phy_in_range_len_errors": 0, "rx_phy_symbol_errors": 0, "rx_phy_discard_packets": 0, "tx_phy_discard_packets": 0, "tx_phy_errors": 0, "rx_out_of_buffer": 78892319, "tx_pp_missed_interrupt_errors": 0, "tx_pp_rearm_queue_errors": 0, "tx_pp_clock_queue_errors": 0, "tx_pp_timestamp_past_errors": 0, "tx_pp_timestamp_future_errors": 0, "tx_pp_jitter": 0, "tx_pp_wander": 0, "tx_pp_sync_lost": 0 } }&lt;/code&gt;&lt;/pre&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/18/statistics-update-open-vswitch-user-space-datapath" title="A statistics update in Open vSwitch user space datapath"&gt;A statistics update in Open vSwitch user space datapath&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>David Marchand</dc:creator><dc:date>2023-09-18T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #36 - September</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-36/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-36/</id><updated>2023-09-15T00:00:00Z</updated><published>2023-09-15T00:00:00Z</published><summary type="html">Explore how we can use the Testcontainers Desktop app while building a Quarkus application by reading "Joyful Quarkus Application Development using Testcontainers Desktop" by Siva Katamreddy. Extensions can significantly increase the application’s performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2023-09-15T00:00:00Z</dc:date></entry><entry><title>Quarkus security releases for CVE-2023-4853</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/cve-2023-4853/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/cve-2023-4853/</id><updated>2023-09-14T00:00:00Z</updated><published>2023-09-14T00:00:00Z</published><summary type="html">We have just released updates to Quarkus 2.16.11.Final, 3.2.6.Final, and 3.3.3 and Red Hat build of Quarkus 2.13.18.SP2 that fix the issue reported in CVE-2023-4853. This issue affects anyone using HTTP security path-based rules to protect HTTP endpoints. Recommendations If you are using any older versions of Quarkus (ranging from...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-14T00:00:00Z</dc:date></entry></feed>
